{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tqdm\nimport os\nimport random\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torchaudio\nimport torch.nn.functional as F\nimport torchaudio.transforms as TT\nimport  matplotlib.pyplot as plt\n\n\nfrom glob import glob\nfrom math import sqrt\nfrom torch.nn import Linear, Conv1d, ConvTranspose2d, SiLU\nfrom torch.utils.data.distributed import DistributedSampler","metadata":{"execution":{"iopub.status.busy":"2023-09-27T06:03:36.860615Z","iopub.execute_input":"2023-09-27T06:03:36.861036Z","iopub.status.idle":"2023-09-27T06:03:40.054283Z","shell.execute_reply.started":"2023-09-27T06:03:36.861002Z","shell.execute_reply":"2023-09-27T06:03:40.053262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUDIO_LEN = 22050*5\nCROP_MEL_FRAMES = 62\nNOISE_SCHEDULE = np.linspace(1e-4, 0.05, 50)\n\nLR = 5e-5\nLOSS_FN = nn.MSELoss()\nEPOCHS = 100\nSAVE_PER_EPOCH = 5\nFILTER_SIZE = 64\nRES_LAYERS = 30\n\nSAVE_DIR = \"./training/\"\nLOAD_PATH = None","metadata":{"execution":{"iopub.status.busy":"2023-09-27T06:03:40.058518Z","iopub.execute_input":"2023-09-27T06:03:40.058925Z","iopub.status.idle":"2023-09-27T06:03:40.065468Z","shell.execute_reply.started":"2023-09-27T06:03:40.058897Z","shell.execute_reply":"2023-09-27T06:03:40.064580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, path, cond=True):\n        super().__init__()\n        self.cond = cond\n        self.filenames = []\n        for wav_file in glob(f'{path}/*.wav'):\n            self.filenames.append(wav_file)\n\n    def __len__(self):\n        return len(self.filenames)\n\n    def __getitem__(self, idx):\n        audio_filename = self.filenames[idx]\n        audio, rate = torchaudio.load(audio_filename)\n        audio = torch.clamp(audio[0], -1.0, 1.0)\n        \n        mel_args = {\n          'sample_rate': rate,\n          'win_length': 256 * 4,\n          'hop_length': 256,\n          'n_fft': 1024,\n          'f_min': 20.0,\n          'f_max': rate / 2.0,\n          'n_mels': 80,\n          'power': 1.0,\n          'normalized': True,\n        }\n        mel_spec_transform = TT.MelSpectrogram(**mel_args)\n\n        with torch.no_grad():\n            spectrogram = mel_spec_transform(audio)\n            spectrogram = 20 * torch.log10(torch.clamp(spectrogram, min=1e-5)) - 20\n            spectrogram = torch.clamp((spectrogram + 100) / 100, 0.0, 1.0)\n\n        if self.cond:\n            return {\n                'audio': audio,\n                'spectrogram': spectrogram.T\n            }\n\n        return {\n                'audio': audio,\n                'spectrogram': None\n            }\n    \nclass Collator():\n    def __init__(self, cond=True):\n        self.cond = cond\n\n    def collate(self, minibatch):\n        samples_per_frame = 256\n        \n        for record in minibatch:\n            if not self.cond:\n                # Filter out records that aren't long enough.\n                if len(record['audio']) < AUDIO_LEN:\n                    del record['spectrogram']\n                    del record['audio']\n                    continue\n\n                start = random.randint(0, record['audio'].shape[-1] - AUDIO_LEN)\n                end = start + AUDIO_LEN\n                record['audio'] = record['audio'][start:end]\n                record['audio'] = np.pad(record['audio'], (0, (end - start) - len(record['audio'])), mode='constant')\n            else:\n                # Filter out records that aren't long enough.\n                if len(record['spectrogram']) < CROP_MEL_FRAMES:\n                    del record['spectrogram']\n                    del record['audio']\n                    continue\n\n                start = random.randint(0, record['spectrogram'].shape[0] - CROP_MEL_FRAMES)\n                end = start + CROP_MEL_FRAMES\n                record['spectrogram'] = record['spectrogram'][start:end].T\n\n                start *= samples_per_frame\n                end *= samples_per_frame\n                record['audio'] = record['audio'][start:end]\n                record['audio'] = np.pad(record['audio'], (0, (end-start) - len(record['audio'])), mode='constant')\n\n        \n        audio = np.stack([record['audio'] for record in minibatch if 'audio' in record])\n        \n        if not self.cond:\n            return {\n            'audio': torch.from_numpy(audio),\n            'spectrogram': None\n            }\n        \n        spectrogram = np.stack([record['spectrogram'] for record in minibatch if 'spectrogram' in record])\n        return {\n            'audio': torch.from_numpy(audio),\n            'spectrogram': torch.from_numpy(spectrogram)\n        }\n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-27T06:03:40.067257Z","iopub.execute_input":"2023-09-27T06:03:40.067929Z","iopub.status.idle":"2023-09-27T06:03:40.088389Z","shell.execute_reply.started":"2023-09-27T06:03:40.067895Z","shell.execute_reply":"2023-09-27T06:03:40.087412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\npath = \"/kaggle/input/the-lj-speech-dataset/LJSpeech-1.1/wavs\"\n\ndataset = Dataset(path)\ntrain_data = torch.utils.data.DataLoader(\n      dataset,\n      batch_size=16,\n      collate_fn=Collator().collate,\n      shuffle= True,\n      pin_memory=True,\n      drop_last=True)\n\ndel dataset\ngc.collect()\n\ntest_audio = next(iter(train_data))\nplt.plot(test_audio['audio'][0])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T06:03:40.091171Z","iopub.execute_input":"2023-09-27T06:03:40.091529Z","iopub.status.idle":"2023-09-27T06:03:45.457123Z","shell.execute_reply.started":"2023-09-27T06:03:40.091496Z","shell.execute_reply":"2023-09-27T06:03:45.456155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Conv1d(*args, **kwargs):\n    layer = nn.Conv1d(*args, **kwargs)\n    layer = nn.utils.weight_norm(layer)\n    nn.init.kaiming_normal_(layer.weight)\n    return layer\n\nclass DiffusionEmbedding(nn.Module):\n    '''\n    Positional Encoding\n    detail could refer to:\n    https://arxiv.org/abs/1706.03762 and https://arxiv.org/abs/2009.09761\n    '''\n    def __init__(self, max_steps):\n        super().__init__()\n        self.register_buffer('embedding', self._build_embedding(max_steps), persistent=False)\n        self.projection1 = Linear(128, 512)\n        self.projection2 = Linear(512, 512)\n\n    def forward(self, diffusion_step):\n        if diffusion_step.dtype in [torch.int32, torch.int64]:\n            x = self.embedding[diffusion_step]\n        else:\n            x = self._lerp_embedding(diffusion_step)\n            \n        x = self.projection1(x)\n        x = SiLU()(x)\n        x = self.projection2(x)\n        x = SiLU()(x)\n        return x\n\n    def _lerp_embedding(self, t):\n        low_idx = torch.floor(t).long()\n        high_idx = torch.ceil(t).long()\n        low = self.embedding[low_idx]\n        high = self.embedding[high_idx]\n        return low + (high - low) * (t - low_idx)\n\n    def _build_embedding(self, max_steps):\n        steps = torch.arange(max_steps).unsqueeze(1)  # [T,1]\n        dims = torch.arange(64).unsqueeze(0)          # [1,64]\n        table = steps * 10.0**(dims * 4.0 / 63.0)     # [T,64]\n        table = torch.cat([torch.sin(table), torch.cos(table)], dim=1)\n        return table\n\nclass SpectrogramUpsampler(nn.Module):\n    def __init__(self, n_mels):\n        super().__init__()\n        self.conv1 = ConvTranspose2d(1, 1, [3, 32], stride=[1, 16], padding=[1, 8])\n        self.conv2 = ConvTranspose2d(1, 1,  [3, 32], stride=[1, 16], padding=[1, 8])\n\n    def forward(self, x):\n        x = torch.unsqueeze(x, 1)\n        x = self.conv1(x)\n        x = F.leaky_relu(x, 0.4)\n        x = self.conv2(x)\n        x = F.leaky_relu(x, 0.4)\n        x = torch.squeeze(x, 1)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-09-27T06:03:45.458707Z","iopub.execute_input":"2023-09-27T06:03:45.459059Z","iopub.status.idle":"2023-09-27T06:03:45.475068Z","shell.execute_reply.started":"2023-09-27T06:03:45.459026Z","shell.execute_reply":"2023-09-27T06:03:45.474157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ResBlock(nn.Module):\n    def __init__(self, res_channel, dilation, n_mels, cond=True):\n        super().__init__()\n        self.dilated_conv = Conv1d(res_channel, 2 * res_channel, 3,\\\n                                    padding=dilation, dilation=dilation)\n        self.diffstep_proj = Linear(512, res_channel)\n        self.cond_proj = Conv1d(n_mels, 2 * res_channel, 1)\n        self.output_proj =  Conv1d(res_channel, 2 * res_channel, 1)\n        self.cond = cond\n\n    def forward(self, inp, diff_step, conditioner):\n        diff_step = self.diffstep_proj(diff_step).unsqueeze(-1)\n        x = inp + diff_step\n        \n        if self.cond:\n            conditioner = self.cond_proj(conditioner)\n            x = self.dilated_conv(x) + conditioner\n        gate, val = torch.chunk(x, 2, dim=1) # gate function\n        x = torch.sigmoid(gate) * torch.tanh(val)\n        \n        x = self.output_proj(x)\n        residual, skip = torch.chunk(x, 2, dim=1)\n        return (inp + residual) / np.sqrt(2.0), skip\n\n\nclass DiffWave(nn.Module):\n    def __init__(self, res_channels, n_layers, n_mels, cond=True):\n        super().__init__()\n        self.cond = cond\n        self.inp_proj = Conv1d(1, res_channels, 1)\n        self.embedding = DiffusionEmbedding(len(NOISE_SCHEDULE))\n        self.spectrogram_upsampler = SpectrogramUpsampler(n_mels)\n        \n        dilate_cycle = n_layers // 3\n        self.layers = nn.ModuleList([\n            ResBlock(res_channels, 2**(i % dilate_cycle), n_mels, self.cond)\n            for i in range(n_layers)\n        ])\n        self.skip_proj = Conv1d(res_channels, res_channels, 1)\n        self.output = Conv1d(res_channels, 1, 1)\n        nn.init.zeros_(self.output.weight)  \n        \n    def forward(self, audio, diffusion_step, spectrogram):\n        x = audio.unsqueeze(1) # (batch_size, 1, audio_sample)\n        x = self.inp_proj(x)\n        x = F.relu(x)\n        diffusion_step = self.embedding(diffusion_step)\n        \n        spectrogram = self.spectrogram_upsampler(spectrogram)\n        if not self.cond:\n            spectrogram = None\n            \n        skip = 0\n        for layer in self.layers:\n            x, skip_connection = layer(x, diffusion_step, spectrogram)\n            skip += skip_connection\n            \n        x = skip / np.sqrt(len(self.layers))\n        x = self.skip_proj(x)\n        x = F.relu(x)\n        x = self.output(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-09-27T06:03:45.476691Z","iopub.execute_input":"2023-09-27T06:03:45.477041Z","iopub.status.idle":"2023-09-27T06:03:45.499749Z","shell.execute_reply.started":"2023-09-27T06:03:45.477009Z","shell.execute_reply":"2023-09-27T06:03:45.498629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DDPM(nn.Module):\n    def __init__(self, model, device):\n        super().__init__()\n        self.model = model\n        self.device = device\n        self.beta = NOISE_SCHEDULE\n        self.alpha = 1 - self.beta\n        self.alpha_bar = np.cumprod(self.alpha, 0)\n        \n    def forward(self, audio, t, noise):\n        # xt = x0 * alpha_bar_sqrt + one_minus_alpha_bar * noise \n\n        alpha_bar = torch.tensor(self.alpha_bar[t], device = self.device, \\\n                                 dtype = torch.float32).unsqueeze(1)\n        alpha_bar_sqrt = alpha_bar ** 0.5\n        one_minus_alpha_bar = (1 - alpha_bar) ** 0.5\n        return alpha_bar_sqrt * audio + one_minus_alpha_bar * noise\n    \n    def reverse(self, x_t, pred_noise, t):\n        alpha_t = np.take(self.alpha, t)\n        alpha_t_bar = np.take(self.alpha_bar, t)\n        \n        mean = (1 / (alpha_t ** 0.5)) * (\n          x_t - (1 - alpha_t) / (1 - alpha_t_bar) ** 0.5 * pred_noise\n        )\n        sigma = np.take(self.beta, t) ** 0.5\n        z = torch.randn_like(x_t)\n        return mean + sigma * z\n    \n    def generate(self, spectrogram):\n        if len(spectrogram.shape) == 2:\n            spectrogram = spectrogram.unsqueeze(0)\n        spectrogram = spectrogram.to(self.device)    \n        x = torch.randn(spectrogram.shape[0], 256 * spectrogram.shape[-1], device=self.device)\n\n        with torch.no_grad():\n            for t in reversed(range(len(self.alpha))):\n                t_tensor = torch.tensor(t, device=self.device).unsqueeze(0)\n                pred_noise = self.model(x, t_tensor, spectrogram).squeeze(1)\n                x = self.reverse(x, pred_noise, t)\n        audio = torch.clamp(x, -1.0, 1.0)\n        return audio\n        ","metadata":{"execution":{"iopub.status.busy":"2023-09-27T06:03:45.501380Z","iopub.execute_input":"2023-09-27T06:03:45.501749Z","iopub.status.idle":"2023-09-27T06:03:45.517331Z","shell.execute_reply.started":"2023-09-27T06:03:45.501716Z","shell.execute_reply":"2023-09-27T06:03:45.516344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport time\nfrom tqdm import *","metadata":{"execution":{"iopub.status.busy":"2023-09-27T06:03:45.518945Z","iopub.execute_input":"2023-09-27T06:03:45.519310Z","iopub.status.idle":"2023-09-27T06:03:45.531513Z","shell.execute_reply.started":"2023-09-27T06:03:45.519277Z","shell.execute_reply":"2023-09-27T06:03:45.530510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Trainer():\n    def __init__(self, model, dataloader, ckpt_dir, \\\n                 epochs, save_n_epoch, diff_method, load_path=None):\n        os.makedirs(ckpt_dir, exist_ok=True)\n        \n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.model = model.to(self.device)\n        self.dataloader = dataloader\n        self.ckpt_dir = ckpt_dir\n        \n        self.best_epoch = 1\n        self.start_epoch = 1\n        self.epochs = epochs\n        self.save_n_epoch = save_n_epoch\n        \n        self.diff_method = diff_method(self.model, self.device)\n        self.optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n        self.scaler = torch.cuda.amp.GradScaler()\n        self.loss_fn = LOSS_FN\n        \n        if load_path is not None:\n            self.load_state_dict(load_path)\n            print(\"sucessful load state dict !!!!!!\")\n            print(f\"start from epoch {self.start_epoch}\")\n    \n    def state_dict(self, epoch):\n        return {\n            \"epoch\": epoch,\n            \"model\": self.model.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"scaler\": self.scaler.state_dict()\n        }\n    \n    def load_state_dict(self, path):\n        state_dict = torch.load(path)\n        self.model.load_state_dict(state_dict['model'])\n        self.optimizer.load_state_dict(state_dict['optimizer'])\n        self.scaler.load_state_dict(state_dict['scaler'])\n        self.start_epoch = state_dict['epoch']\n\n    def train(self):\n        for epoch in tqdm(range(self.start_epoch, self.epochs+1), desc=f\"Training progress\"):\n            start = time.time()\n            print(f'Start of epoch {epoch}')\n    \n            for i, audio_data in enumerate(self.dataloader):\n                self.optimizer.zero_grad()\n            \n                audio = audio_data['audio'].to(self.device)\n                spectrogram = audio_data['spectrogram'].to(self.device)\n                noise = torch.randn_like(audio)\n                t = np.random.randint(len(NOISE_SCHEDULE), size=audio.shape[0])\n\n                noised_audio = self.diff_method(audio, t, noise)\n                predict_noise = self.model(noised_audio, \\\n                                torch.tensor(t, device = self.device), spectrogram).squeeze()\n                                \n                loss = self.loss_fn(noise, predict_noise)\n                \n                self.scaler.scale(loss).backward()\n                self.scaler.unscale_(self.optimizer)\n                self.grad_norm = nn.utils.clip_grad_norm_(self.model.parameters(), 1e9)\n                self.scaler.step(self.optimizer)\n                self.scaler.update()\n            \n                if i > 500:\n                    break\n                \n            if self.best_epoch  > loss:\n                torch.save(self.state_dict(epoch), f\"{self.ckpt_dir}/best_epoch.pt\")\n                print(f\"!!!!!!!!!!!!! saving best epoch {epoch} state dict !!!!!!!```````\")\n                self.best_epoch = loss\n                \n            if epoch % self.save_n_epoch == 0:\n                torch.save(self.state_dict(epoch), f\"{self.ckpt_dir}/weight_epoch{epoch}.pt\")\n                print(f\"sucessful saving epoch {epoch} state dict !!!!!!!\")\n                \n            time_minutes = (time.time() - start) / 60\n            print(f\"epoch: {epoch}, loss: {loss.data} ~~~~~~\")\n            print (f'Time taken for epoch {epoch} is {time_minutes:.3f} min\\n') \n            \n        print(\"finish training: ~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n        \n    def generate(self, spectrogram):\n        return self.diff_method.generate(spectrogram)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T06:03:45.533239Z","iopub.execute_input":"2023-09-27T06:03:45.533611Z","iopub.status.idle":"2023-09-27T06:03:45.553794Z","shell.execute_reply.started":"2023-09-27T06:03:45.533555Z","shell.execute_reply":"2023-09-27T06:03:45.552751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DiffWave(FILTER_SIZE, RES_LAYERS, 80)\ntrainer = Trainer(model, train_data, SAVE_DIR, EPOCHS,\\\n                   SAVE_PER_EPOCH, DDPM, LOAD_PATH)\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T06:03:45.557179Z","iopub.execute_input":"2023-09-27T06:03:45.557675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = trainer.generate(test_audio['spectrogram'][0])\nresult = result[0].data.cpu()\nplt.plot(result)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Audio\nAudio(result, rate=22050)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}